
\section{Introduction}

In ordinary intuitionistic logic or $\lambda$-calculus, assumptions or
variables can go unused (weakening), be used in any order (exchange), be
used more than once (contraction), and be used in any position in a
term.  \emph{Substructural} logics, such as linear logic, ordered logic,
relevant logic, and affine logic, omit some of these structural
properties of weakening, exchange, and contraction, while \emph{modal
  logics} place restrictions on where variables may be used---e.g. a
formula $\Bx{} C$ can only be proved using assumptions of $\Bx{} A$,
while an assumption of $\Dia{}{A}$ can only be used when the conclusion
is $\Dia{}{C}$.  Substructural and modal logics have had many
applications to both functional and logic programming (modeling concepts
such state, staging, distribution, and concurrency, to name just a few).

Substructural and modal logics can also be used as \emph{internal
  languages} of categories, which means using an appropriate logical
language to do constructions ``inside'' a particular mathematical
setting.  This often leads to shorter statements than working
``externally''.  For example, to define a function when working
externally in domains, one must first define the underlying
set-theoretic function, and then prove that it is continuous.  But when
using untyped $\lambda$-calculus as an internal language of domains,
there is no need to prove that a function described by a $\lambda$-term
is continuous, because all terms are shown to denote continous functions
once and for all.  Substructural logics extend this idea to various
forms of monoidal categories, while modal logics describe monads and
comonads.  Recently,
\citet{schreibershulman12cohesive,shulman15realcohesion} proposed using
modal operators to add a notion of \emph{cohesion} to homotopy type
theory/univalent foundations~\citep{,voevodsky06homotopy,uf13hott-book}.
Without going into the precise details of this application, the idea is
to add a triple $\sh{} \la \Flat{} \la \Sharp{}$ of type operators,
where for example $\Sharp A$ is a monad (like a modal possibility
$\Diamond$ or $\bigcirc$), $\Flat A$ is a comonad (like a modal
necessity $\Box$), and there is an adjunction structure between them
(e.g. $\flat{A} \to B$ is the same as $A \to \Sharp{B}$).  This raised
the question of how to best add modalities with these properties to type
theory.

Because other similar applications would have different monads and
comonads with different properties, we would like general tools for
going from a semantic situation of interest to a well-behaved logic/type
theory for it, e.g. one with cut and identity admissibility and the
subformula property. In previous work~\citep{ls16adjoint}, we considered
the special case of a single-assumption logic, building most directly on
the adjoint logics of
\citet{benton94mixed,bentonwadler96adjoint,reed09adjoint}.  Here we
extend this previous work to the multi-assumption case.  The resulting
framework is quite general and covers many existing intuitionistic
substructural and modal connectives: cartesian, linear, affine,
relevant, ordered, bunched~\citep{ohearnpym99bunched} and
non-associative products and implications; $n$-linear
variables~\citep{reed08namessubstructural}; the comonadic $\Box$ and
linear exponential $!$ and
subexponentials~\citep{nigammiller09subexponentials,danos+93subexponentials};
monadic $\Diamond$ and $\bigcirc$ modalities; and adjoint logic $F$ and
$G$~\citep{benton94mixed,bentonwadler96adjoint,reed09adjoint}, including
the single-assumption 2-categorical version from our previous
work~\citep{ls16adjoint}.  It also supports variations on these, such as
non-monoidal comonads and non-strong monads.  We prove cut (and
identity) admissibility for the general framework by a simple
structural~\citep{pfenning94cut} proof; this implies cut admissibility
for any logic that can be described in the framework, including all of
the above, as well as any new logics that one designs using it.
%% While it is not too surprising
%% that this is possible, given that cut proofs for these logics all follow
%% a similar template, it is nonetheless satisfying to codify this pattern
%% as an abstraction.

At a high level, the framework expresses the idea that all of the above
logics are a restriction on how variables can be used in ordinary
structural/cartesian proofs.  We express these restrictions using a
first layer of the logic, which is a simple type theory for what we will
call \emph{modes} and \emph{context descriptors}.  The modes are just a
collection of base types, which we write as $p,q,r$, while a context
descriptor is a term built from variables and constants.  The next layer
is the main logic.  Each proposition of the logic is assigned a mode,
and the basic sequent is \seq{x_1 : A_1, \ldots, x_n : A_n}{\alpha}{C},
where if $A_i$ has mode $p_i$, and $C$ has mode $q$, then $\oftp{x_1 :
  p_1,\ldots, x_n : p_n}{\alpha}{q}$.  
%% In a sequent
%% \seq{\Gamma}{\alpha}{A}, the idea is that $\Gamma$ binds some variables
%% for use both in $\alpha$ and in the derivation.  
$\Gamma$ itself behaves like an ordinary structural/cartesian context,
while the substructural and modal aspects are enforced by the
\emph{term} $\alpha$, which describes how the resources from $\Gamma$
are allowed to be used.  For example, in linear logic/ordered logic/BI,
the context is usually taken to be a multiset/list/tree (respectively).
We represent this by a pair of an ordinary structural context $\Gamma$,
together with a term $\alpha$ that describes the multiset or list or
tree structure, labeled with variables from the ordinary context at the
leaves.  We pronounce a sequent \seq{\Gamma}{\alpha}{A} as ``$\Gamma$
proves $A$ \{along,over\} $\alpha$'' or ``$\Gamma$ structured according to
$\alpha$ proves $A$''.

For example, suppose we have one mode $\dsd{n}$, together with a context
descriptor constant
\[
x : \dsd{n}, y:\dsd{n} \vdash x \odot y : \dsd{n}
\]
Then an example sequent
\[
\seq{x:A, y:B, z:C, w:D}{(y \odot x) \odot z}{E}
\]
should be read as saying that we must prove $E$ using the resources $y$
and $x$ and $z$ (but not $w$) according to the particular tree structure
${(y \odot x) \odot z}$.  If we say nothing else, the framework will
treat $\odot$ as describing a non-associative, linear, ordered context:
if we have a product-like type $A \odot B$ internalizing this context
operation,\footnote{We overload binary operations to refer both to
  context descriptors and propositional connectives, because it is clear
  from whether it is applied to variables $x,y,z$ or propositions
  $A,B,C$ which we mean.}  then we will \emph{not} be able to prove
associativity ($(A \odot B) \odot C \dashv\vdash A \odot (B \odot C)$)
or contraction ($A \vdash A \odot A$) or exchange ($A \odot B \vdash B
\odot A$) etc.

To get from this basic structure to linear or affine or relevant or
cartesian logic, we need to add some structural properties to the
context descriptor term $\alpha$.  We analyze structural properties as
\emph{equations}, or more generally \emph{directed transformations}, on
such terms.  For example, to specify linear logic, we will add a unit
element $1 : \dsd{n}$ together with equations making $(\odot,1)$ into a
commutative monoid:
\[
\begin{array}{c}
x \odot (y \odot z) = (x \odot y) \odot z\\
x \odot 1 = x = 1 \odot x\\
x \odot y = y \odot x
\end{array}
\]
so that the context descriptors ignore associativity and order.  To get
BI, we add an additional commutative monoid $(\times,\top)$ (with
weakening and contraction, as discussed below), so that a BI context
tree $(x:A,y:B);(z:C,w:D)$ can be represented by the ordinary context
$x:A,y:B,z:C,w:D$ with the term $(x \odot y) \times (z \odot w)$
describing the tree.  Because the context descriptors are themselves
ordinary structural/cartesian terms, the same variable can occur more
than once or not at all.  A descriptor such as $x \odot x$ captures the
idea that we can use the \emph{same} variable $x$ twice, expressing
$n$-linear types.  Thus, we can express contraction for a particular
context descriptor $\odot$ as an equation $x = x \odot x$ (one use of
$x$ is the same as two, or $\odot$ is an idempotent binary operation).
However, weakening cannot be represented as an equation between context
descriptors: an equation $x = 1$ trivializes the frameworks to ordinary
intuitionistic logic.  Instead, to express weakening, we use a directed
transformation $x \spr 1$, which is oriented to allow throwing away an
allowed use of $x$, but not creating an allowed use from nothing.  We
refer to these as \emph{structural transformations}, to evoke their use
in representing the structural properties of object logics that are
embedded in our framework.  Structural transformations are also used to
describe relationships between adjunctions~\citep{ls16adjoint}.

In summary, to specify a particular substructural or modal logic, one
gives constants generating context descriptors $\alpha$, with equations
$\alpha = \beta$ and transformations $\alpha \spr \beta$ expressing
structural properties.  The main sequent $\seq{\Gamma}{\alpha}{A}$
respects the specified structural properties in the sense that when
$\alpha = \beta$, we regard $\seq{\Gamma}{\alpha}{A}$ and
$\seq{\Gamma}{\beta}{A}$ as the same sequent, while when $\alpha \spr
\beta$, there will be an operation that takes a derivation of
\seq{\Gamma}{\beta}{A} to a derivation of \seq{\Gamma}{\alpha}{A}.

A guiding principle of the framework is a meta-level notion of
\emph{structurality over structurality}.  For example, we always have
\emph{weakening over weakening}: if \seq{\Gamma}{\alpha}{A} then
\seq{\Gamma,y:B}{\alpha}{A}, where $\alpha$ itself is weakened with $y$.
This does not prevent encodings of e.g. linear logic: it is permissible
to weaken a derivation of \seq{\Gamma}{x_1 \odot \ldots \odot x_n}{A}
(``use $x_1$ through $x_n$'') to a derivation of \seq{\Gamma,y:B}{x_1
  \odot \ldots \odot x_n}{A} because the (weakened) context descriptor
still disallows the use of $y$.  Similarly, we always have exchange over
exchange and contraction over contraction.  The identity and and cut
principles are analogous:
\[
\infer{\seq{\Gamma,x:A}{x}{A}}{}
\qquad
\infer{\seq{\Gamma}{\subst{\beta}{\alpha}{x}}{B}}
    {\seq{\Gamma,x:A}{\beta}{B} &
     \seq{\Gamma}{\alpha}{A}}
\]
The identity-over-identity principle says that we should be able to
prove $A$ using exactly an assumption $x:A$.  The cut principle says
that the context descriptor for the result of the cut is the
substitution of the context descriptor used to prove $A$ into the one
used to prove $B$.  For example, together with weakening-over-weakening,
this captures the usual cut principle of linear logic, which says that
cutting $\Gamma,x:A \vdash B$ and $\Delta \vdash A$ yields
$\Gamma,\Delta \vdash B$.  If $\Gamma$ binds $x_1,\ldots,x_n$ and
$\Delta$ binds $y_1,\ldots,y_n$, then we will represent the two
derivations to be cut together by sequents with
\[
\begin{array}{l}
\beta = x_1 \odot \ldots \odot x_n \odot x\\
\alpha = y_1 \odot \ldots \odot y_n
\end{array}
\]
so
\[
\beta[\alpha/x] = x_1 \odot \ldots \odot x_n \odot y_1 \odot \ldots \odot y_n
\]
correctly deletes $x$ and replaces it with the variables from $\Delta$.
Moreover, in more subtle situations such as BI, the substitution will
insert the resources used to prove the cut formula in the correct place
in the tree.

The framework has two main logical connectives.  The first,
\F{\alpha}{\Delta}, generalizes the \dsd{F} of adjoint logic and the
tensor ($\otimes$) of linear logic.  The second,
\U{x.\alpha}{\Delta}{A}, generalizes the $\dsd{G}/\dsd{U}$ of adjoint
logic and the implication $A \lolli B$ of linear logic.  Here $\Delta$
is a context of assumptions $x_i:A_i$, and trivializing the context
descriptors (i.e. adding an equation $\alpha = \beta$ for all $\alpha$
and $\beta$) degenerates $\F{\alpha}{\Delta}$ into the ordinary
intuititionistic product $A_1 \times \ldots \times A_n$, while
\U{x.\alpha}{\Delta}{A} becomes $A_1 \to \ldots \to A_n \to A$.  Though
we do not give a full polarized/focused proof theory in this paper, we
do prove that \dsd{F} is left-invertible and \dsd{U} is
right-invertible, and we conjecture that focusing works with the
polarization that one would expect based on these degeneracies
($\F{\alpha}{\Delta^{\mathord{+}}}^{\mathord{+}}$ and
$\U{x.\alpha}{\Delta^{\mathord{+}}}{A^{\mathord{-}}}^{\mathord{-}}$).
In linear logic terms, our \dsd{F} and \dsd{U} cover both the
multiplicatives and exponentials; additives can be added separately by
essentially the usual rules.

Being a very general theory, our framework treats the structural
properties in a general but na\"ive way, allowing an arbitrary
structural transformation to be applied at the non-invertible rules for
$\dsd{F}$ and $\dsd{U}$ and at the leaves of a derivation.  For specific
embedded logics, there will often be a more refined discipline that
suffices---e.g. for cartesian logic, always contract all assumptions at
in all premises, rather than choosing which assumptions to contract.  We
view our framework as a tool for bridging the gap between an intended
semantic situation such as the cohesion example mentioned above (``a
comonad and a monad which are themselves adjoint'') and a proof theory:
the framework gives \emph{some} proof theory for the semantics, and the
placement of structural rules can then be optimized purely in syntax.
To support this mode of use, we give an equational theory on sequent
derivations that identifies different placements of the same structural
rules.  This equational theory is used to prove correctness of such
optimizations not just at the level of provability, but also identity of
derivations---which matters for our intended applications to internal
languages.

Semantically, the logic corresponds to a functor between
\emph{2-dimensional cartesian multicategories} which is a fibration in
various senses.  Multicategories are a generalization of categories
which allow more than one object in the domain, and cartesianness means
that the multiple domain objects are treated structurally.  The
2-dimensionality supplies a notion of morphism between (multi)morphisms,
which correspond to the structural transformations.  The functor
specifies the mode of each proposition and the context descriptor of a
sequent.  The fibration conditions (similar to \citep{hermida,hormann})
specify respect for the structural transformations and the presence of
\dsd{F} and \dsd{U} types.

The remainder of this paper is organized as follows.  FIXME

FIXME: comparison with display logic, L/CLF, what else?  

