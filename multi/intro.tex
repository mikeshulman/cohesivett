
\section{Introduction}

In ordinary intuitionistic logic or $\lambda$-calculus, assumptions or
variables can go unused (weakening), be used in any order (exchange), be
used more than once (contraction), and be used in any position in a
term.  \emph{Substructural} logics, such as linear logic, ordered logic,
relevant logic, and affine logic, omit some of these structural
properties of weakening, exchange, and contraction, while \emph{modal
  logics} place restrictions on where variables may be used---e.g. a
formula $\Bx{} C$ can only be proved using assumptions of $\Bx{} A$,
while an assumption of $\Dia{}{A}$ can only be used when the conclusion
is $\Dia{}{C}$.  Substructural and modal logics have had many
applications to both functional and logic programming (modeling concepts
such state, staging, distribution, and concurrency, to name just a few).
Substructural and modal logics are also used as \emph{internal
  languages} of categories, which means using an appropriate logical
language to do constructions ``inside'' a particular mathematical
setting.  This often leads to shorter statements than working
``externally''.  For example, to define a function when working
externally in domains, one must first define the underlying
set-theoretic function, and then prove that it is continuous.  But when
using untyped $\lambda$-calculus as an internal language of domains,
there is no need to prove that a function described by a $\lambda$-term
is continuous, because all terms are shown to denote continuous
functions once and for all.  Substructural logics extend this idea to
various forms of monoidal categories, while modal logics describe monads
and comonads.  Recently,
\citet{schreibershulman12cohesive,shulman15realcohesion} proposed using
modal operators to add a notion of \emph{cohesion} to homotopy type
theory/univalent foundations~\citep{voevodsky06homotopy,uf13hott-book}.
Without going into the precise details of this application, the idea is
to add a triple $\sh{} \la \Flat{} \la \Sharp{}$ of type operators,
where for example $\Sharp{}$ is a monad (like a modal possibility
$\Diamond$ or $\bigcirc$), $\Flat{}$ is a comonad (like a modal
necessity $\Box$), and there is an adjunction structure between them
(e.g. $\flat{A} \to B$ is the same as $A \to \Sharp{B}$).  This raised
the question of how to best add modalities with these properties to type
theory.

Because other similar applications have different monads and comonads
with different properties, we would like general tools for going from a
semantic situation of interest to a well-behaved logic/type theory for
it, e.g. one with cut and identity admissibility (normalization,
$\eta$-expansion).  In previous work~\citep{ls16adjoint}, we considered
the special case of a single-assumption logic, building most directly on
the adjoint logics of
\citet{benton94mixed,bentonwadler96adjoint,reed09adjoint}.  Here we
extend this previous work to the multi-assumption case.  The resulting
framework is quite general and covers many existing intuitionistic
substructural and modal connectives: cartesian, linear, affine,
relevant, ordered, bunched~\citep{ohearnpym99bunched} and
non-associative products and implications; $n$-linear
variables~\citep{reed08namessubstructural,abel15modal,mcbride16nuttin};
the comonadic $\Box$ and linear exponential $!$ and
subexponentials~\citep{nigammiller09subexponentials,danos+93subexponentials};
monadic $\Diamond$ and $\bigcirc$ modalities; and adjoint logic $F$ and
$G$~\citep{benton94mixed,bentonwadler96adjoint,reed09adjoint}, including
the single-assumption 2-categorical version from our previous
work~\citep{ls16adjoint}.  It also supports variations on these, such as
non-monoidal comonads and non-strong monads.

A central syntactic result is that cut (and identity) are admissible for
the general framework itself, by a simple
structural~\citep{pfenning94cut} proof.  This implies cut admissibility
for any logic that can be described in the framework, including all of
the above, as well as any new logics that one designs using it.  When we
view the derivations in the framework as terms in a type theory, this
gives an immediate normalization (and $\eta$-expansion) result.
%% While it is not too surprising
%% that this is possible, given that cut proofs for these logics all follow
%% a similar template, it is nonetheless satisfying to codify this pattern
%% as an abstraction.

At a high level, the framework is based on the idea that all of the
above logics / type theories are a restriction on how variables can be
used in ordinary structural/cartesian proofs.  We express these
restrictions using a first layer, which is a simple type theory for what
we will call \emph{modes} and \emph{context descriptors}.  The modes are
just a collection of base types, which we write as $p,q,r$, while a
context descriptor $\alpha$ is a term built from variables and
constants.  The next layer is the main logic.  Each proposition/type is
assigned a mode, and the basic sequent is \seq{x_1 : A_1, \ldots, x_n :
  A_n}{\alpha}{C}, where if $A_i$ has mode $p_i$, and $C$ has mode $q$,
then $\oftp{x_1 : p_1,\ldots, x_n : p_n}{\alpha}{q}$.  $\Gamma$ itself
behaves like an ordinary structural/cartesian context, while the
substructural and modal aspects are enforced by the \emph{term}
$\alpha$, which describes how the resources from $\Gamma$ are allowed to
be used.  For example, in linear logic/ordered logic/BI, the context is
usually taken to be a multiset/list/tree (respectively).  We represent
this by a pair of an ordinary structural context $\Gamma$, together with
a term $\alpha$ that describes the multiset or list or tree structure,
labeled with variables from the ordinary context at the leaves.  We
pronounce a sequent \seq{\Gamma}{\alpha}{A} as ``$\Gamma$ proves $A$
\{along,over\} $\alpha$'' or ``$\Gamma$ structured according to $\alpha$
proves $A$''.

For example, suppose we have one mode $\dsd{n}$, together with a context
descriptor constant $x : \dsd{n}, y:\dsd{n} \vdash x \odot y : \dsd{n}$.
Then an example sequent \seq{x:A, y:B, z:C, w:D}{(y \odot x) \odot z}{E}
should be read as saying that we must prove $E$ using the resources $y$
and $x$ and $z$ (but not $w$) according to the particular tree structure
${(y \odot x) \odot z}$.  If we say nothing else, the framework will
treat $\odot$ as describing a non-associative, linear, ordered context
as in Lambek calculus~\citep{lambek58calculus}: if we have a
product-like type $A \odot B$ internalizing this context
operation,\footnote{We overload binary operations to refer both to
  context descriptors and propositional connectives, because it is clear
  from whether it is applied to variables $x,y,z$ or propositions
  $A,B,C$ which we mean.}  then we will \emph{not} be able to prove
associativity ($(A \odot B) \odot C \dashv\vdash A \odot (B \odot C)$)
or exchange ($A \odot B \vdash B \odot A$) etc.  

To get from this basic structure to a linear or affine or relevant or
cartesian system, we provide a way to add structural properties governing
the context descriptor term $\alpha$.  We analyze structural properties
as \emph{equations}, or more generally \emph{directed transformations},
on such terms.  For example, to specify linear logic, we will add a unit
element $1 : \dsd{n}$ together with equations making $(\odot,1)$ into a
commutative monoid ($x \odot (y \odot z) = (x \odot y) \odot z$ and 
$x \odot 1 = x = 1 \odot x$ and 
$x \odot y = y \odot x$)
so that the context descriptors ignore associativity and order.  To get
BI, we add an additional commutative monoid $(\times,\top)$ (with
weakening and contraction, as discussed below), so that a BI context
tree $(x:A,y:B);(z:C,w:D)$ can be represented by the ordinary context
$x:A,y:B,z:C,w:D$ with the term $(x \odot y) \times (z \odot w)$
describing the tree.  Because the context descriptors are themselves
ordinary structural/cartesian terms, the same variable can occur more
than once or not at all.  A descriptor such as $x \odot x$ captures the
idea that we can use the \emph{same} variable $x$ twice, expressing
$n$-linear types.  Thus, we can express contraction for a particular
context descriptor $\odot$ as a transformation $x \spr x \odot x$ (one
use of $x$ allows two).  Weakening, on the other hand, is represented by
a transformation $x \spr 1$, which is oriented to allow throwing away an
allowed use of $x$, but not creating an allowed use from nothing.  We
refer to these as \emph{structural transformations}, to evoke their use
in representing the structural properties of object logics that are
embedded in our framework.  The main sequent $\seq{\Gamma}{\alpha}{A}$
respects the specified structural properties in the sense that when
$\alpha = \beta$, we regard $\seq{\Gamma}{\alpha}{A}$ and
$\seq{\Gamma}{\beta}{A}$ as the same sequent, while when $\alpha \spr
\beta$, there will be an operation that takes a derivation of
\seq{\Gamma}{\beta}{A} to a derivation of \seq{\Gamma}{\alpha}{A}.

Modal logics will generally involve a mode theory with more than one
mode.  For example, a context descriptor $x : \dsd{c} \vdash \dsd{f}(x)
: \dsd{l}$ will generate an adjoint pair of functors between the two
modes, as in the adjoint syntax for the $!$ modality of linear
logic~\citep{bentonwadler96adjoint} or other modal
operators~\citep{reed09adjoint}.  Structural transformations are used to
describe how these modal operators interact with each other and with the
product structures, and in some cases~\citep{ls16adjoint} it is
important that there can be more than one structural transformation
between a given pair of context descriptors.

A guiding principle of the framework is a meta-level notion of
\emph{structurality over structurality}.  For example, we always have
\emph{weakening over weakening}: if \seq{\Gamma}{\alpha}{A} then
\seq{\Gamma,y:B}{\alpha}{A}, where $\alpha$ itself is weakened with $y$.
This does not prevent encodings of e.g. linear logic: it is permissible
to weaken a derivation of \seq{\Gamma}{x_1 \odot \ldots \odot x_n}{A}
(``use $x_1$ through $x_n$'') to a derivation of \seq{\Gamma,y:B}{x_1
  \odot \ldots \odot x_n}{A} because the (weakened) context descriptor
still disallows the use of $y$.  Similarly, we always have exchange over
exchange and contraction over contraction.  The identity and and cut
principles are analogous.  The identity-over-identity principle says
that we should be able to prove $A$ using exactly an assumption $x:A$:
{\seq{\Gamma,x:A}{x}{A}}.  The cut principle says that from
\seq{\Gamma,x:A}{\beta}{B} and \seq{\Gamma}{\alpha}{A} we get
{\seq{\Gamma}{\subst{\beta}{\alpha}{x}}{B}}---the context descriptor
for the result of the cut is the substitution of the context
descriptor used to prove $A$ into the one used to prove $B$.  For
example, together with weakening-over-weakening, this captures the
usual cut principle of linear logic, which says that cutting
$\Gamma,x:A \vdash B$ and $\Delta \vdash A$ yields $\Gamma,\Delta
\vdash B$.  If $\Gamma$ binds $x_1,\ldots,x_n$ and $\Delta$ binds
$y_1,\ldots,y_n$, then we will represent the two derivations to be
cut together by sequents with $\beta = x_1 \odot \ldots \odot x_n
\odot x$ and $\alpha = y_1 \odot \ldots \odot y_n$ then
$\beta[\alpha/x] = x_1 \odot \ldots \odot x_n \odot y_1 \odot \ldots \odot y_n$
correctly deletes $x$ and replaces it with the variables from $\Delta$.
In more subtle situations such as BI, the substitution will
insert the resources used to prove the cut formula in the correct place
in the tree.

The framework has two main logical connectives / type constructors.  The
first, \F{\alpha}{\Delta}, generalizes the \dsd{F} of adjoint logic and
the multiplicative products (e.g. $\otimes$ of linear logic).  The
second, \U{x.\alpha}{\Delta}{A}, generalizes the $\dsd{G}/\dsd{U}$ of
adjoint logic and implication (e.g. $A \lolli B$ in linear logic).  Here
$\Delta$ is a context of assumptions $x_i:A_i$, and trivializing the
context descriptors (i.e. adding an equation $\alpha = \beta$ for all
$\alpha$ and $\beta$) degenerates $\F{\alpha}{\Delta}$ into the ordinary
intuitionistic product $A_1 \times \ldots \times A_n$, while
\U{x.\alpha}{\Delta}{A} becomes $A_1 \to \ldots \to A_n \to A$.  Though
we do not give a polarized/focused proof theory in this paper, we do
prove that \dsd{F} is left-invertible and \dsd{U} is right-invertible.
%%  and we conjecture that focusing works with the
%% polarization that one would expect based on these degeneracies
%% ($\F{\alpha}{\Delta^{\mathord{+}}}^{\mathord{+}}$ and
%% $\U{x.\alpha}{\Delta^{\mathord{+}}}{A^{\mathord{-}}}^{\mathord{-}}$).
In linear logic terms, our \dsd{F} and \dsd{U} cover both the
multiplicatives and exponentials; additives can be added separately by
the usual rules.  

In summary, to specify a particular substructural or modal logic / type
theory, one gives constants generating context descriptors $\alpha$,
with equations $\alpha = \beta$ and transformations $\alpha \spr \beta$
expressing structural properties.  We discuss many examples of
\emph{logical adequacy} theorems, showing that a sequent can be proved
in a standard sequent calculus for a logic iff its embedding can be
proved in the framework.  

Being a very general theory, our framework treats the object-logic
structural properties in a general but na\"ive way, allowing an
arbitrary structural transformation to be applied at the non-invertible
rules for $\dsd{F}$ and $\dsd{U}$ and at the leaves of a derivation.
For specific embedded logics, there will often be a more refined
discipline that suffices---e.g. for cartesian logic, always contract all
assumptions in all premises, rather than choosing which assumptions to
contract.  We view our framework as a tool for bridging the gap between
an intended semantic situation such as the cohesion example mentioned
above (``a comonad and a monad which are themselves adjoint'') and a
proof theory: the framework gives \emph{some} proof theory for the
semantics, and the placement of structural rules can then be optimized
purely in syntax.  To support this mode of use, we give an equational
theory on sequent derivations that identifies different placements of
the same structural rules.  This equational theory can be used to prove
correctness of such optimizations not just at the level of provability,
but also identity of derivations---which matters for our intended
applications to internal languages.  We discuss some preliminary work on
\emph{equational adequacy}, which extends the logical correspondence to
isomorphisms of definitional-equality-classes of derivations.

Semantically, the logic corresponds to a functor between
\emph{2-dimensional cartesian multicategories} which is a fibration in
various senses.  Multicategories are a generalization of categories
which allow more than one object in the domain, and cartesianness means
that the multiple domain objects are treated structurally.  The
2-dimensionality supplies a notion of morphism between (multi)morphisms.
A \emph{mode theory} specifying context descriptors and structural
properties is analyzed as a cartesian 2-multicategory, with the
descriptors as 1-cells and the structural properties as 2-cells.  The
functor relates the sequent judgement to the mode theory, specifying the
mode of each proposition and the context descriptor of a sequent.  The
fibration conditions (similar to
\citep{hermida02fibrations,hormann15multicategories}) give respect for
the structural transformations and the presence of \dsd{F} and \dsd{U}
types.  We prove that the sequent calculus and the equational theory are
sound and complete for this semantics: the syntax can be interpreted in
any bifibration, and itself determines one.  This semantics shows that a
class of interesting type theories can be identified with a class of
mathematical objects, fibrations of cartesian 2-multicategories, thus
providing a partial answer to the question of characterizing what a
substructural or modal type theory is in mathematical terms.

Our framework builds on many approaches to substructural and modal logic
in the literature; we mention but a few of the most closely related
systems.  Logical rules that act at a leaf of a tree-structured context
go back to the Lambek calculus~\citep{lambek58calculus}.  A rich
collection of context structures that correspond to type constructors
plays a central role in display logic~\citep{belnap82display}.
\citet{atkey04separation}'s $\lambda$-calculus for resource separation
corresponds to the case of our framework with one mode, where there is
at most one 2-cell between a given pair of 1-cells; at the logical
level, our calculus is a unification of this with the multimode adjoint
logic of \citet{reed09adjoint}.  Algebraic resource annotations on
variables are used to track modalities in Agda's
implementation~\citep{abel15modal} and in \citet{mcbride16nuttin}'s
approach to linear dependent types.  LF representations of modal or
substructural logics work by restricting the use of cartesian
variables~\citep{crary10substructural}.  Relative to all of these
approaches, we believe that the analysis of the context
structures/resources as a \emph{term} in a base type theory, and the
fibrational structure of the derivations over them, is a new and useful
observation.  For example, rather than needing extra-logical conditions
on proof rules to ensure cut admissibility, as in display logic, the
conditions are encoded in the language of context descriptors and the
definition of types from them.  Moreover, none of these existing
approaches allow for proof-relevant 2-cells/structural rules, and their
presence (and the equational theory we give for them) is important for
our applications to extensions of homotopy type theory.  A point of
contrast with substructural logical
frameworks~\citep{cervesatopfenning02llf,watkins+03clf-tr,reed09thesis}
is that logics are ``embedded'' in our calculus (giving a type
translation such that provability in the object logic corresponds to
provability in ours), rather than ``encoding'' the structure of
derivations as a type.  This way, we obtain cut elimination for object
languages as a corollary of cut elimination for the framework.
%% Similar semantic structures have come up recently
%% in~\citep{zeilberger,mellieszeilberger,johann}.  
 
